# 2. –ò–º–ø–æ—Ä—Ç—ã
import os
import glob
import pandas as pd
import torch
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
from transformers import DebertaV2Tokenizer, AutoModelForSequenceClassification
from collections import Counter

# 3. –ù–∞—Å—Ç—Ä–æ–π–∫–∏
SAVE_DIR = "nyt_2024_sections"
MODEL_NAME = "microsoft/deberta-v3-small"  # –ú–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –ª—é–±—É—é –¥—Ä—É–≥—É—é
CHUNK_SIZE = 500  # –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤ –≤ —á–∞–Ω–∫–µ
BATCH_SIZE = 128  # –ë–∞—Ç—á —Ç–µ–ø–µ—Ä—å –æ–≥—Ä–æ–º–Ω—ã–π, –ø–æ—Ç–æ–º—É —á—Ç–æ GPU –±—É–¥–µ—Ç —Å–ø—Ä–∞–≤–ª—è—Ç—å—Å—è

# 4. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ —É—Å—Ç—Ä–æ–π—Å—Ç–≤—É
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"üöÄ –†–∞–±–æ—Ç–∞–µ–º –Ω–∞ {device}")

# 5. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
csv_files = glob.glob(os.path.join(SAVE_DIR, "*.csv"))
df_list = [pd.read_csv(file) for file in csv_files]
all_news_df = pd.concat(df_list, ignore_index=True)

all_news_df['published_date'] = pd.to_datetime(all_news_df['published_date']).dt.date
assert 'full_text' in all_news_df.columns, "–û—à–∏–±–∫–∞: –Ω–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ 'full_text'."
print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_news_df)} –Ω–æ–≤–æ—Å—Ç–µ–π.")

# 6. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å
print(f"üß† –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å {MODEL_NAME}...")
tokenizer = DebertaV2Tokenizer.from_pretrained(MODEL_NAME)

model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
).to(device)
model.eval() # –ü–µ—Ä–µ–≤–æ–¥–∏–º –≤ —Ä–µ–∂–∏–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞

# 7. –§—É–Ω–∫—Ü–∏—è –¥–µ–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏
def chunk_text_by_tokens(text, tokenizer, max_tokens=500):
    if pd.isnull(text) or not text.strip():
        return []
    tokens = tokenizer(text, return_tensors=None, add_special_tokens=False)["input_ids"]
    chunks = []
    for i in range(0, len(tokens), max_tokens):
        chunk_ids = tokens[i:i+max_tokens]
        chunk = tokenizer.decode(chunk_ids, skip_special_tokens=True)
        chunks.append(chunk)
    return chunks

# 8. –ì–æ—Ç–æ–≤–∏–º –≤—Å–µ —á–∞–Ω–∫–∏
print("‚úÇÔ∏è –†–µ–∂–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –Ω–∞ —á–∞–Ω–∫–∏...")
all_chunks = []
chunk_to_article_idx = []

for idx, text in tqdm(enumerate(all_news_df['full_text']), total=len(all_news_df)):
    chunks = chunk_text_by_tokens(text, tokenizer, max_tokens=CHUNK_SIZE)
    all_chunks.extend(chunks)
    chunk_to_article_idx.extend([idx] * len(chunks))

print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(all_chunks)} —á–∞–Ω–∫–æ–≤.")

# 9. –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –∏ –∑–∞–≥—Ä—É–∑—á–∏–∫
class TextChunksDataset(Dataset):
    def __init__(self, texts, tokenizer, max_length=512):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoding = self.tokenizer(
            self.texts[idx],
            truncation=True,
            padding="max_length",
            max_length=self.max_length,
            return_tensors="pt"
        )
        return {key: val.squeeze(0) for key, val in encoding.items()}

dataset = TextChunksDataset(all_chunks, tokenizer)
dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)

# 10. –ü—Ä–æ–≥–æ–Ω—è–µ–º —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å
print("üöÄ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ —á–µ—Ä–µ–∑ DataLoader...")
all_preds = []

with torch.no_grad():
    for batch in tqdm(dataloader):
        batch = {key: val.to(device) for key, val in batch.items()}
        outputs = model(**batch)
        probs = outputs.logits.softmax(dim=-1)
        preds = probs.argmax(dim=-1)
        all_preds.extend(preds.cpu().tolist())

# 11. –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ª–µ–π–±–ª—ã
id2label = {0: "negative", 1: "neutral", 2: "positive"}
chunk_sentiments = [id2label.get(pred, "neutral") for pred in all_preds]

# 12. –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
print("üîß –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã...")

article_sentiments = [[] for _ in range(len(all_news_df))]
for idx, sentiment in zip(chunk_to_article_idx, chunk_sentiments):
    article_sentiments[idx].append(sentiment)

final_sentiments = []
for sentiments in article_sentiments:
    if not sentiments:
        final_sentiments.append('neutral')
        continue
    counter = Counter(sentiments)
    most_common = counter.most_common(1)[0][0]
    final_sentiments.append(most_common)

all_news_df['sentiment'] = final_sentiments

# 13. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
output_file = f"nyt_2024_news_sentiment_{MODEL_NAME.split('/')[-1]}_turbo.csv"
all_news_df[['published_date', 'section', 'title', 'url', 'sentiment', 'full_text']].to_csv(output_file, index=False)

print(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {output_file}.")
